import os
import wandb
wandb.init(mode="disabled")
from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer
from transformers import AutoTokenizer
import torch
from datasets import load_dataset


# your student model
tokenizer = AutoTokenizer.from_pretrained("/home/jcyao/mrc_correction_llama/llama2_13b_t5_3b/t5-3b")
model = AutoModelForSeq2SeqLM.from_pretrained("/home/jcyao/mrc_correction_llama/llama2_13b_t5_3b/t5-3b")

max_input_length = 2048
max_target_length = 1024


def preprocess_function(examples):
    input_context = [con for con in examples['context']]
    input_question = [que for que in examples['question']]
    input_options = [opt for opt in examples['options']]
    inputs = [str(context) + str(question) + str(options)
              for context, question, options in zip(input_context, input_question, input_options)]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Set up the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        output_rationale = [ration for ration in examples['rationale']]
        output_answer = [ans for ans in examples['answer']]
        outputs = [str(answer) + str(rationale) for answer, rationale in zip(output_answer, output_rationale)]
        labels = tokenizer(outputs, max_length=max_target_length, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


device = torch.device('cuda' if torch.cuda.is_available else 'cpu')
print(device)
data_files = {'train': '../RACE_PLUS/train/train_4_2.csv',
              'test': '../RACE_PLUS/test/test_2.csv', 'val': '../RACE_PLUS/dev/dev_2.csv'}  # training data generated by teacher model
dataset = load_dataset('csv', data_files=data_files)

raw_datasets = dataset
tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)

batch_size = 1  # 4
args = Seq2SeqTrainingArguments(
    output_dir="t5-ft-et",
    num_train_epochs=10,
    do_train=True,
    do_eval=True,
    per_device_train_batch_size=batch_size,  # demo
    per_device_eval_batch_size=batch_size,
    learning_rate=1e-04,
    warmup_steps=500,
    weight_decay=0.001,
    label_smoothing_factor=0.1,
    predict_with_generate=True,
    logging_dir="logs",
    logging_steps=100,  # 1000
    evaluation_strategy="epoch",
    save_total_limit=3,

    generation_max_length=1024,
    # generation_num_beams=3
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["val"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    # compute_metrics=compute_metrics
)

trainer.train()
trainer.save_model()
trainer.save_state()
